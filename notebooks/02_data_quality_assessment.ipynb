{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Market Trend Analysis - Phase 1\n",
    "## Notebook 2: Data Quality Assessment\n",
    "\n",
    "**Author:** Enerita  \n",
    "**Date:** December 2025  \n",
    "**Project:** Stock Market Trend Analysis - Global Investment Partners  \n",
    "**Purpose:** Comprehensive data quality analysis and issue resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/raw/\"\n",
    "\n",
    "company_info = pd.read_csv(data_path + 'company_info.csv')\n",
    "stock_prices = pd.read_csv(data_path + 'stock_prices.csv')\n",
    "stock_indicators = pd.read_csv(data_path + 'stock_prices_with_indicators.csv')\n",
    "market_indices = pd.read_csv(data_path + 'market_indices.csv')\n",
    "\n",
    "print(\"All datasets loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Missing Values Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing Values Summary\\n\")\n",
    "\n",
    "datasets = {\n",
    "    'Company Info': company_info,\n",
    "    'Stock Prices': stock_prices,\n",
    "    'Stock Indicators': stock_indicators,\n",
    "    'Market Indices': market_indices\n",
    "}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    total_cells = df.shape[0] * df.shape[1]\n",
    "    missing_cells = df.isnull().sum().sum()\n",
    "    missing_pct = (missing_cells / total_cells) * 100\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Total cells: {total_cells:,}\")\n",
    "    print(f\"  Missing cells: {missing_cells:,}\")\n",
    "    print(f\"  Missing percentage: {missing_pct:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Detailed Missing Values by Column (Stock Indicators)\\n\")\n",
    "\n",
    "missing_indicators = stock_indicators.isnull().sum()\n",
    "missing_pct_indicators = (missing_indicators / len(stock_indicators)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing_indicators.index,\n",
    "    'Missing_Count': missing_indicators.values,\n",
    "    'Missing_Percentage': missing_pct_indicators.values\n",
    "})\n",
    "\n",
    "missing_df = missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False)\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    print(missing_df.to_string(index=False))\n",
    "    \n",
    "    os.makedirs('../visuals/eda', exist_ok=True)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    top_missing = missing_df.head(15)\n",
    "    plt.barh(top_missing['Column'], top_missing['Missing_Percentage'])\n",
    "    plt.xlabel('Missing Percentage (%)')\n",
    "    plt.title('Top 15 Columns with Missing Values')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../visuals/eda/missing_values.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"\\nVisualization saved: visuals/eda/missing_values.png\")\n",
    "else:\n",
    "    print(\"No missing values found in primary dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Data Type Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Stock Indicators Data Types:\\n\")\n",
    "print(stock_indicators.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Converting date columns to datetime\\n\")\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    if 'date' in df.columns:\n",
    "        original_type = df['date'].dtype\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        print(f\"{name}: {original_type} -> datetime64\")\n",
    "\n",
    "stock_prices['date'] = pd.to_datetime(stock_prices['date'])\n",
    "stock_indicators['date'] = pd.to_datetime(stock_indicators['date'])\n",
    "market_indices['date'] = pd.to_datetime(market_indices['date'])\n",
    "\n",
    "print(\"\\nAll date columns converted to datetime format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Numeric Columns Verification\\n\")\n",
    "\n",
    "numeric_cols = stock_indicators.select_dtypes(include=[np.number]).columns\n",
    "print(f\"Total numeric columns: {len(numeric_cols)}\")\n",
    "print(f\"Examples: {list(numeric_cols[:10])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Duplicate Records Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Duplicate Records Check\\n\")\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Total records: {len(df):,}\")\n",
    "    print(f\"  Duplicate records: {duplicates:,}\")\n",
    "    print(f\"  Duplicate percentage: {(duplicates/len(df)*100):.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ticker-Date Combination Duplicates\\n\")\n",
    "\n",
    "ticker_date_duplicates = stock_indicators.duplicated(subset=['ticker', 'date']).sum()\n",
    "print(f\"Stock Indicators: {ticker_date_duplicates} ticker-date duplicates\")\n",
    "\n",
    "if ticker_date_duplicates > 0:\n",
    "    print(\"\\nExample duplicates:\")\n",
    "    print(stock_indicators[stock_indicators.duplicated(subset=['ticker', 'date'], keep=False)].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Date Range and Continuity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Date Range Coverage\\n\")\n",
    "\n",
    "date_datasets = {\n",
    "    'Stock Prices': stock_prices,\n",
    "    'Stock Indicators': stock_indicators,\n",
    "    'Market Indices': market_indices\n",
    "}\n",
    "\n",
    "for name, df in date_datasets.items():\n",
    "    min_date = df['date'].min()\n",
    "    max_date = df['date'].max()\n",
    "    date_range_days = (max_date - min_date).days\n",
    "    unique_dates = df['date'].nunique()\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Start date: {min_date.date()}\")\n",
    "    print(f\"  End date: {max_date.date()}\")\n",
    "    print(f\"  Date range: {date_range_days} days\")\n",
    "    print(f\"  Unique trading days: {unique_dates}\")\n",
    "    print(f\"  Coverage: {(unique_dates/date_range_days*100):.1f}% of calendar days\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Records per Stock\\n\")\n",
    "\n",
    "records_per_ticker = stock_indicators.groupby('ticker')['date'].count().sort_values(ascending=False)\n",
    "print(f\"Mean records per stock: {records_per_ticker.mean():.0f}\")\n",
    "print(f\"Min records: {records_per_ticker.min()}\")\n",
    "print(f\"Max records: {records_per_ticker.max()}\")\n",
    "print(f\"Std deviation: {records_per_ticker.std():.0f}\")\n",
    "\n",
    "print(\"\\nStocks with varying record counts:\")\n",
    "print(records_per_ticker.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Date Gap Analysis (per ticker)\\n\")\n",
    "\n",
    "sample_tickers = stock_indicators['ticker'].unique()[:3]\n",
    "for ticker in sample_tickers:\n",
    "    ticker_data = stock_indicators[stock_indicators['ticker'] == ticker].sort_values('date')\n",
    "    date_diffs = ticker_data['date'].diff().dt.days\n",
    "    gaps = date_diffs[date_diffs > 7]\n",
    "    \n",
    "    print(f\"{ticker}:\")\n",
    "    print(f\"  Total records: {len(ticker_data)}\")\n",
    "    print(f\"  Date gaps > 7 days: {len(gaps)}\")\n",
    "    if len(gaps) > 0:\n",
    "        print(f\"  Max gap: {gaps.max()} days\\n\")\n",
    "    else:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Price Data Outliers\\n\")\n",
    "\n",
    "negative_prices = (stock_indicators[['open', 'high', 'low', 'close', 'volume']] < 0).sum()\n",
    "print(\"Negative values found:\")\n",
    "if negative_prices.sum() > 0:\n",
    "    print(negative_prices[negative_prices > 0])\n",
    "else:\n",
    "    print(\"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_indicators['daily_return'] = stock_indicators.groupby('ticker')['close'].pct_change()\n",
    "extreme_returns = stock_indicators[abs(stock_indicators['daily_return']) > 0.5]\n",
    "\n",
    "print(f\"\\nExtreme daily returns (>50% change): {len(extreme_returns)}\")\n",
    "if len(extreme_returns) > 0:\n",
    "    print(\"\\nExample extreme returns:\")\n",
    "    print(extreme_returns[['ticker', 'date', 'close', 'daily_return']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Technical Indicator Outliers\\n\")\n",
    "\n",
    "if 'rsi_14' in stock_indicators.columns:\n",
    "    rsi_outliers = stock_indicators[(stock_indicators['rsi_14'] < 0) | (stock_indicators['rsi_14'] > 100)]\n",
    "    print(f\"RSI outside 0-100 range: {len(rsi_outliers)}\")\n",
    "\n",
    "Q1 = stock_indicators['volume'].quantile(0.25)\n",
    "Q3 = stock_indicators['volume'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "volume_outliers = stock_indicators[(stock_indicators['volume'] < Q1 - 1.5*IQR) | \n",
    "                                   (stock_indicators['volume'] > Q3 + 1.5*IQR)]\n",
    "print(f\"Volume outliers (IQR method): {len(volume_outliers)} ({len(volume_outliers)/len(stock_indicators)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'trend_label' in stock_indicators.columns:\n",
    "    print(\"Trend Label Distribution\\n\")\n",
    "    \n",
    "    trend_counts = stock_indicators['trend_label'].value_counts()\n",
    "    trend_pcts = stock_indicators['trend_label'].value_counts(normalize=True) * 100\n",
    "    \n",
    "    trend_summary = pd.DataFrame({\n",
    "        'Count': trend_counts,\n",
    "        'Percentage': trend_pcts\n",
    "    })\n",
    "    print(trend_summary)\n",
    "    \n",
    "    max_class_pct = trend_pcts.max()\n",
    "    min_class_pct = trend_pcts.min()\n",
    "    imbalance_ratio = max_class_pct / min_class_pct\n",
    "    \n",
    "    print(f\"\\nClass Imbalance Ratio: {imbalance_ratio:.2f}\")\n",
    "    if imbalance_ratio > 2:\n",
    "        print(\"WARNING: Significant class imbalance detected\")\n",
    "        print(\"Recommendation: Consider using class weights or resampling techniques\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    trend_counts.plot(kind='bar', color=['green', 'red', 'gray'])\n",
    "    plt.title('Target Variable Distribution (Trend Labels)')\n",
    "    plt.xlabel('Trend Label')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../visuals/eda/target_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"\\nVisualization saved: visuals/eda/target_distribution.png\")\n",
    "    \n",
    "    print(\"\\nMissing Target Values\")\n",
    "    missing_target = stock_indicators['trend_label'].isnull().sum()\n",
    "    print(f\"Missing trend labels: {missing_target} ({missing_target/len(stock_indicators)*100:.2f}%)\")\n",
    "    \n",
    "else:\n",
    "    print(\"WARNING: Target variable 'trend_label' not found in dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Data Quality Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data Quality Summary Report\\n\")\n",
    "\n",
    "quality_issues = []\n",
    "\n",
    "total_missing = stock_indicators.isnull().sum().sum()\n",
    "quality_issues.append({\n",
    "    'Check': 'Missing Values',\n",
    "    'Status': 'PASS' if total_missing == 0 else 'ATTENTION',\n",
    "    'Details': f'{total_missing:,} missing cells',\n",
    "    'Action': 'Handle with forward fill' if total_missing > 0 else 'None required'\n",
    "})\n",
    "\n",
    "duplicates = stock_indicators.duplicated().sum()\n",
    "quality_issues.append({\n",
    "    'Check': 'Duplicate Records',\n",
    "    'Status': 'PASS' if duplicates == 0 else 'FAIL',\n",
    "    'Details': f'{duplicates} duplicates',\n",
    "    'Action': 'Remove duplicates' if duplicates > 0 else 'None required'\n",
    "})\n",
    "\n",
    "min_records = records_per_ticker.min()\n",
    "max_records = records_per_ticker.max()\n",
    "quality_issues.append({\n",
    "    'Check': 'Date Coverage',\n",
    "    'Status': 'ATTENTION' if max_records - min_records > 50 else 'PASS',\n",
    "    'Details': f'Records vary from {min_records} to {max_records}',\n",
    "    'Action': 'Document variations' if max_records - min_records > 50 else 'None required'\n",
    "})\n",
    "\n",
    "if 'trend_label' in stock_indicators.columns:\n",
    "    missing_target = stock_indicators['trend_label'].isnull().sum()\n",
    "    quality_issues.append({\n",
    "        'Check': 'Target Variable',\n",
    "        'Status': 'PASS' if missing_target == 0 else 'ATTENTION',\n",
    "        'Details': f'{missing_target} missing labels',\n",
    "        'Action': 'Remove rows with missing target' if missing_target > 0 else 'None required'\n",
    "    })\n",
    "\n",
    "date_correct = pd.api.types.is_datetime64_any_dtype(stock_indicators['date'])\n",
    "quality_issues.append({\n",
    "    'Check': 'Data Types',\n",
    "    'Status': 'PASS' if date_correct else 'FAIL',\n",
    "    'Details': 'Date column correctly formatted' if date_correct else 'Date needs conversion',\n",
    "    'Action': 'None required' if date_correct else 'Convert to datetime'\n",
    "})\n",
    "\n",
    "quality_df = pd.DataFrame(quality_issues)\n",
    "print(quality_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Recommendations and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Recommendations and Next Steps\\n\")\n",
    "\n",
    "print(\"Data Cleaning Actions Required:\")\n",
    "\n",
    "actions = []\n",
    "if total_missing > 0:\n",
    "    actions.append(\"Handle missing values using forward fill method\")\n",
    "if duplicates > 0:\n",
    "    actions.append(\"Remove duplicate records\")\n",
    "if 'trend_label' in stock_indicators.columns and missing_target > 0:\n",
    "    actions.append(\"Remove rows with missing target variable\")\n",
    "actions.append(\"Verify extreme values and outliers\")\n",
    "actions.append(\"Create clean modeling dataset\")\n",
    "\n",
    "for i, action in enumerate(actions, 1):\n",
    "    print(f\"{i}. {action}\")\n",
    "\n",
    "print(\"\\nData Quality Score\")\n",
    "\n",
    "passed_checks = sum(1 for issue in quality_issues if issue['Status'] == 'PASS')\n",
    "total_checks = len(quality_issues)\n",
    "quality_score = (passed_checks / total_checks) * 100\n",
    "\n",
    "print(f\"Checks passed: {passed_checks}/{total_checks}\")\n",
    "print(f\"Data Quality Score: {quality_score:.0f}%\")\n",
    "\n",
    "if quality_score >= 80:\n",
    "    print(\"Data quality is GOOD - proceed to EDA\")\n",
    "elif quality_score >= 60:\n",
    "    print(\"Data quality is ACCEPTABLE - address issues before modeling\")\n",
    "else:\n",
    "    print(\"Data quality is POOR - significant cleanup required\")\n",
    "\n",
    "print(\"\\nReady for Next Step:\")\n",
    "print(\"Proceed to Notebook 03: Exploratory Data Analysis\")\n",
    "print(\"After addressing the data quality issues identified above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_report = {\n",
    "    'assessment_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'total_records': len(stock_indicators),\n",
    "    'total_features': stock_indicators.shape[1],\n",
    "    'missing_values': total_missing,\n",
    "    'duplicates': duplicates,\n",
    "    'quality_score': quality_score,\n",
    "    'recommendations': actions\n",
    "}\n",
    "\n",
    "print(\"\\nQuality assessment complete\")\n",
    "print(\"Data quality report ready for documentation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
